{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Dissection: Exploring Llama-3-8B Architecture\n",
    "====================================================\n",
    "\n",
    "Interactive exploration of the model's internal structure.\n",
    "Convert to .ipynb for cell-by-cell execution.\n",
    "\n",
    "Sections:\n",
    "1. Model Loading\n",
    "2. Architecture Overview\n",
    "3. Layer Inspection\n",
    "4. Attention Mechanism Deep Dive\n",
    "5. Weight Analysis\n",
    "6. Forward Hook Demonstration\n",
    "7. Token Flow Visualization\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb176460",
   "metadata": {},
   "source": [
    "# Model Dissection: Llama-3-8B Architecture\n",
    "\n",
    "This notebook explores every component of the Llama model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5fdc8",
   "metadata": {
    "title": "[1] Imports and Setup"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import gc\n",
    "\n",
    "# Enforce CUDA\n",
    "assert torch.cuda.is_available(), \"CUDA required!\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc1496",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[2] Load Model"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7d1a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Architecture Overview\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d332e06",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[3] Print Full Architecture"
   },
   "outputs": [],
   "source": [
    "def print_model_architecture(model):\n",
    "    \"\"\"Print the complete model architecture.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPLETE MODEL ARCHITECTURE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(model)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Uncomment to see full architecture (very long output)\n",
    "# print_model_architecture(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ea2b8",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[4] Model Configuration"
   },
   "outputs": [],
   "source": [
    "def print_model_config(model):\n",
    "    \"\"\"Print key model configuration parameters.\"\"\"\n",
    "    config = model.config\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL CONFIGURATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    key_params = [\n",
    "        (\"Model Type\", config.model_type),\n",
    "        (\"Hidden Size (d_model)\", config.hidden_size),\n",
    "        (\"Intermediate Size (FFN)\", config.intermediate_size),\n",
    "        (\"Num Attention Heads\", config.num_attention_heads),\n",
    "        (\"Num Key-Value Heads\", config.num_key_value_heads),\n",
    "        (\"Num Hidden Layers\", config.num_hidden_layers),\n",
    "        (\"Vocab Size\", config.vocab_size),\n",
    "        (\"Max Position Embeddings\", config.max_position_embeddings),\n",
    "        (\"RoPE Theta\", getattr(config, 'rope_theta', 'N/A')),\n",
    "        (\"RMS Norm Epsilon\", config.rms_norm_eps),\n",
    "        (\"Tie Word Embeddings\", config.tie_word_embeddings),\n",
    "    ]\n",
    "    \n",
    "    for name, value in key_params:\n",
    "        print(f\"{name:30} : {value}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    return config\n",
    "\n",
    "config = print_model_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140545fe",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[5] Count Parameters"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model) -> Dict[str, int]:\n",
    "    \"\"\"Count parameters by component.\"\"\"\n",
    "    param_counts = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        # Get top-level component\n",
    "        parts = name.split('.')\n",
    "        if len(parts) >= 2:\n",
    "            component = f\"{parts[0]}.{parts[1]}\"\n",
    "        else:\n",
    "            component = parts[0]\n",
    "        \n",
    "        if component not in param_counts:\n",
    "            param_counts[component] = 0\n",
    "        param_counts[component] += param.numel()\n",
    "    \n",
    "    return param_counts\n",
    "\n",
    "def print_parameter_summary(model):\n",
    "    \"\"\"Print parameter count summary.\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PARAMETER SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Parameters:     {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Memory (fp16):        {total_params * 2 / 1e9:.2f} GB\")\n",
    "    print(f\"Memory (fp32):        {total_params * 4 / 1e9:.2f} GB\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # By component\n",
    "    param_counts = count_parameters(model)\n",
    "    print(\"\\nBy Component:\")\n",
    "    print(\"-\" * 60)\n",
    "    for component, count in sorted(param_counts.items(), key=lambda x: -x[1]):\n",
    "        percentage = count / total_params * 100\n",
    "        print(f\"{component:40} : {count:>15,} ({percentage:5.2f}%)\")\n",
    "\n",
    "print_parameter_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b094c00",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Layer Structure\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a114261",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[6] Explore Model Layers"
   },
   "outputs": [],
   "source": [
    "def explore_layer_structure(model):\n",
    "    \"\"\"Explore the structure of transformer layers.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRANSFORMER LAYER STRUCTURE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get first layer\n",
    "    layer = model.model.layers[0]\n",
    "    \n",
    "    print(f\"\\nLayer Type: {type(layer).__name__}\")\n",
    "    print(f\"\\nComponents in each layer:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for name, module in layer.named_children():\n",
    "        print(f\"\\n  {name}: {type(module).__name__}\")\n",
    "        \n",
    "        # Go one level deeper\n",
    "        if hasattr(module, 'named_children'):\n",
    "            for subname, submodule in module.named_children():\n",
    "                print(f\"    └── {subname}: {type(submodule).__name__}\")\n",
    "                \n",
    "                # Get shapes if it's a linear layer\n",
    "                if isinstance(submodule, nn.Linear):\n",
    "                    print(f\"        Shape: {submodule.weight.shape}\")\n",
    "\n",
    "explore_layer_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec38c83",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[7] Visualize Layer Dimensions"
   },
   "outputs": [],
   "source": [
    "def visualize_layer_dimensions(model):\n",
    "    \"\"\"Create a visual representation of data flow through a layer.\"\"\"\n",
    "    config = model.config\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA FLOW THROUGH ONE TRANSFORMER LAYER\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    d_model = config.hidden_size\n",
    "    n_heads = config.num_attention_heads\n",
    "    n_kv_heads = config.num_key_value_heads\n",
    "    d_head = d_model // n_heads\n",
    "    d_ff = config.intermediate_size\n",
    "    \n",
    "    flow = f\"\"\"\n",
    "    Input: [batch, seq_len, {d_model}]\n",
    "           │\n",
    "           ▼\n",
    "    ┌──────────────────────────────────────────────────┐\n",
    "    │  INPUT LAYER NORM (RMSNorm)                      │\n",
    "    │  Shape: [{d_model}] → [{d_model}]                │\n",
    "    └──────────────────────────────────────────────────┘\n",
    "           │\n",
    "           ▼\n",
    "    ┌──────────────────────────────────────────────────┐\n",
    "    │  SELF-ATTENTION                                  │\n",
    "    │                                                  │\n",
    "    │  Q projection: [{d_model}] → [{d_model}]         │\n",
    "    │      Reshape: [{n_heads} heads × {d_head} dim]   │\n",
    "    │                                                  │\n",
    "    │  K projection: [{d_model}] → [{n_kv_heads * d_head}]  │\n",
    "    │      (Grouped Query Attention: {n_kv_heads} KV heads) │\n",
    "    │                                                  │\n",
    "    │  V projection: [{d_model}] → [{n_kv_heads * d_head}]  │\n",
    "    │                                                  │\n",
    "    │  Attention: softmax(QK^T / √{d_head}) × V       │\n",
    "    │                                                  │\n",
    "    │  O projection: [{d_model}] → [{d_model}]         │\n",
    "    └──────────────────────────────────────────────────┘\n",
    "           │\n",
    "           ▼\n",
    "    + ────── (Residual Connection)\n",
    "           │\n",
    "           ▼\n",
    "    ┌──────────────────────────────────────────────────┐\n",
    "    │  POST-ATTENTION LAYER NORM (RMSNorm)             │\n",
    "    │  Shape: [{d_model}] → [{d_model}]                │\n",
    "    └──────────────────────────────────────────────────┘\n",
    "           │\n",
    "           ▼\n",
    "    ┌──────────────────────────────────────────────────┐\n",
    "    │  FEED-FORWARD NETWORK (SwiGLU)                   │\n",
    "    │                                                  │\n",
    "    │  gate_proj: [{d_model}] → [{d_ff}]               │\n",
    "    │  up_proj:   [{d_model}] → [{d_ff}]               │\n",
    "    │                                                  │\n",
    "    │  hidden = SiLU(gate) × up                        │\n",
    "    │                                                  │\n",
    "    │  down_proj: [{d_ff}] → [{d_model}]               │\n",
    "    └──────────────────────────────────────────────────┘\n",
    "           │\n",
    "           ▼\n",
    "    + ────── (Residual Connection)\n",
    "           │\n",
    "           ▼\n",
    "    Output: [batch, seq_len, {d_model}]\n",
    "    \"\"\"\n",
    "    print(flow)\n",
    "    \n",
    "    print(\"\\nKey Architecture Notes:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"• Grouped Query Attention (GQA): {n_heads} Q heads, {n_kv_heads} KV heads\")\n",
    "    print(f\"• KV sharing ratio: {n_heads // n_kv_heads}:1\")\n",
    "    print(f\"• FFN expansion ratio: {d_ff / d_model:.1f}x\")\n",
    "    print(f\"• Uses RMSNorm (not LayerNorm)\")\n",
    "    print(f\"• Uses SwiGLU activation (not ReLU/GELU)\")\n",
    "    print(f\"• Uses RoPE for positional encoding\")\n",
    "\n",
    "visualize_layer_dimensions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd6b3e",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Attention Mechanism Deep Dive\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962116e4",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[8] Inspect Attention Module"
   },
   "outputs": [],
   "source": [
    "def inspect_attention_module(model):\n",
    "    \"\"\"Deep dive into the attention mechanism.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ATTENTION MODULE INSPECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    attn = model.model.layers[0].self_attn\n",
    "    config = model.config\n",
    "    \n",
    "    print(f\"\\nAttention Module Type: {type(attn).__name__}\")\n",
    "    print(f\"\\nConfiguration (from model.config):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get values from config (more reliable than module attributes)\n",
    "    n_heads = config.num_attention_heads\n",
    "    n_kv_heads = config.num_key_value_heads\n",
    "    head_dim = config.hidden_size // n_heads\n",
    "    \n",
    "    print(f\"  num_attention_heads: {n_heads}\")\n",
    "    print(f\"  num_key_value_heads: {n_kv_heads}\")\n",
    "    print(f\"  head_dim: {head_dim}\")\n",
    "    print(f\"  hidden_size: {config.hidden_size}\")\n",
    "    print(f\"  num_key_value_groups: {n_heads // n_kv_heads}\")\n",
    "    \n",
    "    print(f\"\\nProjection Weights:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  q_proj: {attn.q_proj.weight.shape} → Q vectors\")\n",
    "    print(f\"  k_proj: {attn.k_proj.weight.shape} → K vectors\")\n",
    "    print(f\"  v_proj: {attn.v_proj.weight.shape} → V vectors\")\n",
    "    print(f\"  o_proj: {attn.o_proj.weight.shape} → Output projection\")\n",
    "    \n",
    "    return attn\n",
    "\n",
    "attn = inspect_attention_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e541e61",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[9] Visualize Q, K, V Projection"
   },
   "outputs": [],
   "source": [
    "def visualize_qkv_projections(model, tokenizer, text: str):\n",
    "    \"\"\"Visualize Q, K, V vectors for a sample input.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Q, K, V PROJECTION VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    seq_len = input_ids.shape[1]\n",
    "    \n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    print(f\"Tokens: {seq_len}\")\n",
    "    print(f\"Token IDs: {input_ids[0].tolist()}\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        hidden_states = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    print(f\"\\nEmbedding shape: {hidden_states.shape}\")\n",
    "    \n",
    "    # Get first layer attention and config\n",
    "    attn = model.model.layers[0].self_attn\n",
    "    config = model.config\n",
    "    \n",
    "    # Get head dimensions from config (not from attn module)\n",
    "    n_heads = config.num_attention_heads\n",
    "    n_kv_heads = config.num_key_value_heads\n",
    "    head_dim = config.hidden_size // n_heads\n",
    "    \n",
    "    # Apply layer norm first (important!)\n",
    "    norm = model.model.layers[0].input_layernorm\n",
    "    with torch.no_grad():\n",
    "        normed = norm(hidden_states)\n",
    "    \n",
    "    # Project to Q, K, V\n",
    "    with torch.no_grad():\n",
    "        Q = attn.q_proj(normed)\n",
    "        K = attn.k_proj(normed)\n",
    "        V = attn.v_proj(normed)\n",
    "    \n",
    "    print(f\"\\nProjection Shapes (before reshape):\")\n",
    "    print(f\"  Q: {Q.shape}\")\n",
    "    print(f\"  K: {K.shape}\")\n",
    "    print(f\"  V: {V.shape}\")\n",
    "    \n",
    "    # Reshape to heads\n",
    "    Q_reshaped = Q.view(1, seq_len, n_heads, head_dim).transpose(1, 2)\n",
    "    K_reshaped = K.view(1, seq_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "    V_reshaped = V.view(1, seq_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "    \n",
    "    print(f\"\\nReshaped to heads:\")\n",
    "    print(f\"  Q: {Q_reshaped.shape} → [batch, {n_heads} heads, seq, {head_dim} dim]\")\n",
    "    print(f\"  K: {K_reshaped.shape} → [batch, {n_kv_heads} KV heads, seq, {head_dim} dim]\")\n",
    "    print(f\"  V: {V_reshaped.shape} → [batch, {n_kv_heads} KV heads, seq, {head_dim} dim]\")\n",
    "    \n",
    "    return Q_reshaped, K_reshaped, V_reshaped\n",
    "\n",
    "Q, K, V = visualize_qkv_projections(model, tokenizer, \"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50578e56",
   "metadata": {
    "title": "[10] Compute Attention Scores Manually"
   },
   "outputs": [],
   "source": [
    "def compute_attention_manually(Q, K, V, layer_idx: int = 0):\n",
    "    \"\"\"Manually compute attention scores step by step.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"MANUAL ATTENTION COMPUTATION (Layer {layer_idx})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get shapes\n",
    "    batch, n_heads, seq_len, head_dim = Q.shape\n",
    "    _, n_kv_heads, _, _ = K.shape\n",
    "    \n",
    "    # GQA: repeat K, V to match Q heads\n",
    "    n_rep = n_heads // n_kv_heads\n",
    "    if n_rep > 1:\n",
    "        K = K.repeat_interleave(n_rep, dim=1)\n",
    "        V = V.repeat_interleave(n_rep, dim=1)\n",
    "        print(f\"GQA: Repeated K,V {n_rep}x to match {n_heads} Q heads\")\n",
    "    \n",
    "    # Step 1: QK^T\n",
    "    print(f\"\\nStep 1: Compute QK^T\")\n",
    "    print(f\"  Q shape: {Q.shape}\")\n",
    "    print(f\"  K^T shape: {K.transpose(-2, -1).shape}\")\n",
    "    \n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    print(f\"  QK^T shape: {scores.shape}\")\n",
    "    \n",
    "    # Step 2: Scale\n",
    "    scale = head_dim ** 0.5\n",
    "    print(f\"\\nStep 2: Scale by √d = √{head_dim} = {scale:.2f}\")\n",
    "    scaled_scores = scores / scale\n",
    "    \n",
    "    # Step 3: Causal mask\n",
    "    print(f\"\\nStep 3: Apply causal mask\")\n",
    "    causal_mask = torch.triu(\n",
    "        torch.ones(seq_len, seq_len, device=Q.device), \n",
    "        diagonal=1\n",
    "    ).bool()\n",
    "    masked_scores = scaled_scores.masked_fill(causal_mask, float('-inf'))\n",
    "    \n",
    "    # Step 4: Softmax\n",
    "    print(f\"\\nStep 4: Softmax → Attention probabilities\")\n",
    "    attn_probs = torch.softmax(masked_scores, dim=-1)\n",
    "    print(f\"  Attention probs shape: {attn_probs.shape}\")\n",
    "    print(f\"  Sum per row (should be 1.0): {attn_probs[0, 0, :, :].sum(dim=-1)}\")\n",
    "    \n",
    "    # Step 5: Weighted sum of V\n",
    "    print(f\"\\nStep 5: Weighted sum of Values\")\n",
    "    output = torch.matmul(attn_probs, V)\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    \n",
    "    return attn_probs, output, causal_mask\n",
    "\n",
    "\n",
    "attn_probs, attn_output, causal_mask = compute_attention_manually(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f320d",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[11] Visualize Attention Patterns"
   },
   "outputs": [],
   "source": [
    "def visualize_attention_pattern(attn_probs, head_idx: int = 0, save_path: str = None):\n",
    "    \"\"\"Visualize attention pattern for a specific head.\"\"\"\n",
    "    # Get attention matrix for one head\n",
    "    attn_matrix = attn_probs[0, head_idx].cpu().float().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(attn_matrix, cmap='viridis', aspect='auto')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    ax.set_title(f'Attention Pattern - Head {head_idx}')\n",
    "    plt.colorbar(im, label='Attention Weight')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Uncomment to visualize:\n",
    "# visualize_attention_pattern(attn_probs, head_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd28db78",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Query Norm & Entropy (Phase 1 Metrics)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a25629",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[12] Compute Query Norm"
   },
   "outputs": [],
   "source": [
    "def compute_query_norms(Q):\n",
    "    \"\"\"\n",
    "    Compute L2 norm of query vectors.\n",
    "    \n",
    "    This is the key metric for Phase 1: we hypothesize that\n",
    "    ‖Q‖ predicts attention entropy.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor [batch, n_heads, seq_len, head_dim]\n",
    "        \n",
    "    Returns:\n",
    "        norms: [batch, n_heads, seq_len]\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUERY NORM COMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # L2 norm along the head_dim axis\n",
    "    norms = torch.norm(Q, p=2, dim=-1)\n",
    "    \n",
    "    print(f\"Q shape: {Q.shape}\")\n",
    "    print(f\"Norm shape: {norms.shape}\")\n",
    "    print(f\"\\nSample norms (head 0):\")\n",
    "    print(f\"  Min: {norms[0, 0].min().item():.4f}\")\n",
    "    print(f\"  Max: {norms[0, 0].max().item():.4f}\")\n",
    "    print(f\"  Mean: {norms[0, 0].mean().item():.4f}\")\n",
    "    print(f\"  Std: {norms[0, 0].std().item():.4f}\")\n",
    "    \n",
    "    return norms\n",
    "\n",
    "q_norms = compute_query_norms(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b01b14",
   "metadata": {
    "title": "[13] Compute Attention Entropy (MASK-AWARE, NaN-SAFE)"
   },
   "outputs": [],
   "source": [
    "def compute_attention_entropy(attn_probs, causal_mask=None, eps: float = 1e-9):\n",
    "    \"\"\"\n",
    "    Mask-aware attention entropy.\n",
    "\n",
    "    Correctly handles:\n",
    "    - causal masking\n",
    "    - zero-probability positions\n",
    "    - fp16 instability\n",
    "\n",
    "    Args:\n",
    "        attn_probs: [batch, n_heads, seq_len, seq_len]\n",
    "        causal_mask: [seq_len, seq_len] with True = masked\n",
    "        eps: numerical stability constant\n",
    "\n",
    "    Returns:\n",
    "        entropy: [batch, n_heads, seq_len]\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ATTENTION ENTROPY COMPUTATION (MASK-AWARE)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Promote to fp32 for numerical stability\n",
    "    attn_probs = attn_probs.float()\n",
    "\n",
    "    batch, n_heads, seq_len, _ = attn_probs.shape\n",
    "\n",
    "    # Build causal mask if not provided\n",
    "    if causal_mask is None:\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=attn_probs.device),\n",
    "            diagonal=1\n",
    "        ).bool()\n",
    "\n",
    "    # valid_mask: True where attention is allowed\n",
    "    valid_mask = ~causal_mask                     # [seq, seq]\n",
    "    valid_mask = valid_mask.unsqueeze(0).unsqueeze(0)  # [1,1,seq,seq]\n",
    "\n",
    "    # Zero out masked probabilities explicitly\n",
    "    masked_probs = attn_probs * valid_mask\n",
    "\n",
    "    # Compute entropy ONLY over valid positions\n",
    "    entropy = -torch.sum(\n",
    "        masked_probs * torch.log(masked_probs + eps),\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    # Optional: ignore trivial early tokens (no context)\n",
    "    entropy[..., :2] = torch.nan\n",
    "\n",
    "    print(f\"Attention probs shape: {attn_probs.shape}\")\n",
    "    print(f\"Entropy shape: {entropy.shape}\")\n",
    "\n",
    "    print(\"\\nSample entropy (head 0):\")\n",
    "    finite_vals = entropy[0, 0][~torch.isnan(entropy[0, 0])]\n",
    "    print(f\"  Min: {finite_vals.min().item():.4f}\")\n",
    "    print(f\"  Max: {finite_vals.max().item():.4f}\")\n",
    "    print(f\"  Mean: {finite_vals.mean().item():.4f}\")\n",
    "\n",
    "    # Theoretical max entropy varies per token\n",
    "    print(\"\\nNote:\")\n",
    "    print(\"• Entropy is computed ONLY over valid causal support\")\n",
    "    print(\"• Early tokens are ignored (NaN by design)\")\n",
    "    print(\"• No log(0), no NaNs\")\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19621053",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[14] Correlate Query Norm with Entropy (NaN-safe)"
   },
   "outputs": [],
   "source": [
    "def correlate_norm_entropy(q_norms, attn_entropy):\n",
    "    \"\"\"\n",
    "    Compute correlation between query norm and attention entropy.\n",
    "    NaN-safe and statistically correct.\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUERY NORM ↔ ATTENTION ENTROPY CORRELATION (NaN-SAFE)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    n_heads = q_norms.shape[1]\n",
    "\n",
    "    print(f\"\\nPer-head correlations:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for h in range(n_heads):\n",
    "        norms = q_norms[0, h].cpu().float().numpy()\n",
    "        entropy = attn_entropy[0, h].cpu().float().numpy()\n",
    "\n",
    "        # ✅ DROP NaNs\n",
    "        valid = ~np.isnan(entropy)\n",
    "        norms = norms[valid]\n",
    "        entropy = entropy[valid]\n",
    "\n",
    "        # ⚠️ Need at least 4 points for correlation\n",
    "        if len(norms) < 4:\n",
    "            pearson_r = spearman_r = np.nan\n",
    "            pearson_p = spearman_p = np.nan\n",
    "        else:\n",
    "            pearson_r, pearson_p = stats.pearsonr(norms, entropy)\n",
    "            spearman_r, spearman_p = stats.spearmanr(norms, entropy)\n",
    "\n",
    "        results.append({\n",
    "            'head': h,\n",
    "            'pearson_r': pearson_r,\n",
    "            'pearson_p': pearson_p,\n",
    "            'spearman_r': spearman_r,\n",
    "            'spearman_p': spearman_p,\n",
    "        })\n",
    "\n",
    "        if h < 5 or h >= n_heads - 2:\n",
    "            print(\n",
    "                f\"  Head {h:2d}: \"\n",
    "                f\"Pearson r={pearson_r:+.4f}, \"\n",
    "                f\"Spearman ρ={spearman_r:+.4f}\"\n",
    "            )\n",
    "        elif h == 5:\n",
    "            print(f\"  ... ({n_heads - 7} more heads)\")\n",
    "\n",
    "attn_entropy = compute_attention_entropy(attn_probs, causal_mask)    \n",
    "\n",
    "# This will work with more tokens, but let's demonstrate structure\n",
    "correlations = correlate_norm_entropy(q_norms, attn_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59690c0e",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Forward Hooks (How We'll Capture Data)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae67cc",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[15] Hook Demonstration"
   },
   "outputs": [],
   "source": [
    "class AttentionHook:\n",
    "    \"\"\"\n",
    "    Hook to capture Q vectors and attention probabilities during forward pass.\n",
    "    \n",
    "    This is how we'll collect data for the full experiment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.captured_data = {}\n",
    "        self.hooks = []\n",
    "    \n",
    "    def create_hook(self, layer_idx: int):\n",
    "        \"\"\"Create a hook function for a specific layer.\"\"\"\n",
    "        def hook_fn(module, input, output):\n",
    "            # Note: The exact structure depends on the model implementation\n",
    "            # This is a template - adjust based on actual model outputs\n",
    "            self.captured_data[layer_idx] = {\n",
    "                'output': output,\n",
    "                'module_type': type(module).__name__,\n",
    "            }\n",
    "            print(f\"  Hook fired: Layer {layer_idx}\")\n",
    "        return hook_fn\n",
    "    \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register hooks on all attention layers.\"\"\"\n",
    "        print(\"Registering attention hooks...\")\n",
    "        \n",
    "        for idx, layer in enumerate(model.model.layers):\n",
    "            hook = layer.self_attn.register_forward_hook(self.create_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "        \n",
    "        print(f\"Registered {len(self.hooks)} hooks\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        print(\"All hooks removed\")\n",
    "    \n",
    "    def clear_data(self):\n",
    "        \"\"\"Clear captured data.\"\"\"\n",
    "        self.captured_data = {}\n",
    "\n",
    "def demonstrate_hooks(model, tokenizer):\n",
    "    \"\"\"Demonstrate hook functionality.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FORWARD HOOK DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    hook_manager = AttentionHook()\n",
    "    hook_manager.register_hooks(model)\n",
    "    \n",
    "    # Run forward pass\n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    inputs = tokenizer(\"Test\", return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    print(f\"\\nCaptured data from {len(hook_manager.captured_data)} layers\")\n",
    "    \n",
    "    # Cleanup\n",
    "    hook_manager.remove_hooks()\n",
    "    hook_manager.clear_data()\n",
    "    \n",
    "    return hook_manager\n",
    "\n",
    "# Uncomment to test:\n",
    "# hook_manager = demonstrate_hooks(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ab04b",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Weight Statistics\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1bd59",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[16] Analyze Weight Distributions"
   },
   "outputs": [],
   "source": [
    "def analyze_weight_distributions(model, layer_idx: int = 0):\n",
    "    \"\"\"Analyze weight distributions for a layer.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"WEIGHT DISTRIBUTION ANALYSIS (Layer {layer_idx})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    weight_stats = []\n",
    "    for name, param in layer.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            w = param.data.float().cpu().numpy().flatten()\n",
    "            stats = {\n",
    "                'name': name,\n",
    "                'shape': tuple(param.shape),\n",
    "                'mean': np.mean(w),\n",
    "                'std': np.std(w),\n",
    "                'min': np.min(w),\n",
    "                'max': np.max(w),\n",
    "            }\n",
    "            weight_stats.append(stats)\n",
    "    \n",
    "    print(f\"\\n{'Name':<40} {'Shape':<20} {'Mean':>10} {'Std':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for s in weight_stats:\n",
    "        print(f\"{s['name']:<40} {str(s['shape']):<20} {s['mean']:>10.4f} {s['std']:>10.4f}\")\n",
    "    \n",
    "    return weight_stats\n",
    "\n",
    "weight_stats = analyze_weight_distributions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79248a1c",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[17] Visualize Weight Distribution"
   },
   "outputs": [],
   "source": [
    "def plot_weight_histogram(model, layer_idx: int = 0, proj_name: str = \"q_proj\"):\n",
    "    \"\"\"Plot weight histogram for a specific projection.\"\"\"\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    attn = layer.self_attn\n",
    "    \n",
    "    proj = getattr(attn, proj_name)\n",
    "    weights = proj.weight.data.float().cpu().numpy().flatten()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.hist(weights, bins=100, density=True, alpha=0.7)\n",
    "    ax.set_xlabel('Weight Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Weight Distribution: Layer {layer_idx} {proj_name}')\n",
    "    ax.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Uncomment to visualize:\n",
    "# plot_weight_histogram(model, layer_idx=0, proj_name=\"q_proj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00a11b",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: RoPE (Rotary Position Embedding)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd12fa5",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[18] Understand RoPE"
   },
   "outputs": [],
   "source": [
    "def explain_rope(model):\n",
    "    \"\"\"Explain Rotary Position Embeddings.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ROTARY POSITION EMBEDDING (RoPE)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config = model.config\n",
    "    \n",
    "    print(\"\"\"\n",
    "    RoPE encodes position by rotating Q and K vectors.\n",
    "    \n",
    "    Key idea: \n",
    "    - Instead of adding position embeddings, we ROTATE the vectors\n",
    "    - Rotation angle depends on position and dimension\n",
    "    - Inner product Q·K naturally encodes relative position\n",
    "    \n",
    "    Benefits:\n",
    "    - No absolute position limit (can extrapolate)\n",
    "    - Relative position is captured\n",
    "    - No learned position parameters needed\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"\\nRoPE Configuration:\")\n",
    "    print(f\"  Base frequency (theta): {getattr(config, 'rope_theta', 10000)}\")\n",
    "    print(f\"  Max position: {config.max_position_embeddings}\")\n",
    "\n",
    "explain_rope(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17b85d",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Summary & Cleanup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455005b",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[19] Summary"
   },
   "outputs": [],
   "source": [
    "def print_summary(model):\n",
    "    \"\"\"Print a summary of key findings.\"\"\"\n",
    "    config = model.config\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL DISSECTION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Model: {config._name_or_path}\n",
    "    \n",
    "    Architecture Highlights:\n",
    "    ─────────────────────────\n",
    "    • {config.num_hidden_layers} transformer layers\n",
    "    • {config.hidden_size} hidden dimension\n",
    "    • {config.num_attention_heads} attention heads\n",
    "    • {config.num_key_value_heads} KV heads (GQA)\n",
    "    • {config.intermediate_size} FFN dimension\n",
    "    \n",
    "    Key Components:\n",
    "    ─────────────────────────\n",
    "    • Attention: Grouped Query Attention (GQA)\n",
    "    • Normalization: RMSNorm\n",
    "    • Activation: SwiGLU\n",
    "    • Position: RoPE\n",
    "    \n",
    "    Phase 1 Metrics:\n",
    "    ─────────────────────────\n",
    "    • Query Norm: ‖Q‖₂ computed per (layer, head, token)\n",
    "    • Attention Entropy: H = -Σ p log(p)\n",
    "    • Hypothesis: corr(‖Q‖, H) < 0\n",
    "    \n",
    "    Next Steps:\n",
    "    ─────────────────────────\n",
    "    1. Register hooks to capture Q and attention probs\n",
    "    2. Run inference on long-context input\n",
    "    3. Collect (layer, head, token, q_norm, entropy) data\n",
    "    4. Compute Pearson/Spearman correlations\n",
    "    5. Visualize results\n",
    "    \"\"\")\n",
    "\n",
    "print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15c90c",
   "metadata": {
    "title": "[20] Cleanup"
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"\"\"Clean up GPU memory.\"\"\"\n",
    "    global model, tokenizer, Q, K, V, attn_probs, attn_output, q_norms, attn_entropy\n",
    "    \n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared.\")\n",
    "\n",
    "# Uncomment when done:\n",
    "# cleanup()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
